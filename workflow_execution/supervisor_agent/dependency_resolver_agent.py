"""
Module Name: dependency_resolver_agent.py

Description:
This module provides functionality to resolve dependencies between tasks and structure 
input for subsequent tasks. The module defines the input/output models, loads prompts, 
and processes task dependencies using a language model.

"""

# -----------------------------------------------------------------------------
# SECTION: Imports
# -----------------------------------------------------------------------------

# Standard library imports
from typing import List,Dict,Any,Union
import json

# Third-party imports
from langchain_community.callbacks import get_openai_callback
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel

# Local application imports
from models.openai.azure_openai_model import llm_config_loader
from utils.helper_functions import load_prompt, load_prompt_yaml

# -----------------------------------------------------------------------------
# SECTION: Load Prompts
# -----------------------------------------------------------------------------

# Load the dependency resolver prompt from a file
dependency_resolver_system_text = load_prompt_yaml(r"config_files\core_engine\dependency_resolver_agent\system_prompt.yaml")

# Create chat prompt template
dependency_resolver_chat_prompt = ChatPromptTemplate.from_messages(
    [("system", dependency_resolver_system_text)]
)

def fix_dict_json(json_str):
    """
    Fix Python-style dictionary strings in JSON to proper JSON format.
    
    Args:
        json_str (str): A string containing JSON with possible Python dict syntax
        
    Returns:
        str: Properly formatted JSON string
    """
    import re
    
    # Replace Python single quotes with double quotes
    # This regex finds single quotes not preceded by a backslash
    fixed = re.sub(r"(?<!\\)'", '"', json_str)
    return fixed

json_fixer = RunnableLambda(fix_dict_json)


def parse_fixed_json(fixed_json_str):
    try:
        return json.loads(fixed_json_str)
    except json.JSONDecodeError as e:
        print(f"JSON decode error after fixing: {e}")
        # Return a minimal valid structure if parsing fails
        return {"function_params": {}}

json_parser = RunnableLambda(parse_fixed_json)
# -----------------------------------------------------------------------------
# SECTION: Define Models
# -----------------------------------------------------------------------------

# Define the Pydantic models
class DependencyResolverInput(BaseModel):
    """
    Input model for the dependency resolver.

    Attributes:
        previous_output (str): The output from the previous task.
        next_task_info (dict): The next task information containing function params.
    """
    previous_output: str
    next_task_info: dict


class DependencyResolverOutput(BaseModel):
    """
    Output model for the dependency resolver.

    Attributes:
        function_params (List[str]): Structured input parameters for the next task.
    """
    function_params: Union[Dict[str, Any], List[Any]]

# -----------------------------------------------------------------------------
# SECTION: Initialize Output Parser and Chain
# -----------------------------------------------------------------------------

# Create the JSON output parser
# dependency_parser = JsonOutputParser(pydantic_object=DependencyResolverOutput)

# Create the chain
dependency_resolver_chain = dependency_resolver_chat_prompt | llm_config_loader() | StrOutputParser() | json_fixer | json_parser 

# -----------------------------------------------------------------------------
# SECTION: Define Dependency Resolver Function
# -----------------------------------------------------------------------------

def dependency_resolver(previous_output: str, next_task_info: dict) -> tuple:
    """
    Process and structure the output for the next task.

    This function takes the output from a previous task and the next task information,
    and uses the dependency resolver chain to produce structured input for the next task.

    Args:
        previous_output (str): The output from the previous task.
        next_task_info (dict): The next task information containing function params.

    Returns:
        tuple: A tuple containing:
            - List[str]: Structured input parameters for the next task.
            - int: Count of input tokens used by the AI model.
            - int: Count of output tokens generated by the AI model.
    """
    with get_openai_callback() as cb:
        
        ai_response = dependency_resolver_chain.invoke({
            "previous_output": previous_output,
            "next_task_info": next_task_info
        })
    
    print("AI Response dependency_resolver: ", ai_response)
    structured_input = ai_response['function_params']
    input_tokens_count = cb.prompt_tokens
    output_tokens_count = cb.completion_tokens
    return structured_input, input_tokens_count, output_tokens_count

# -----------------------------------------------------------------------------
# END OF MODULE
# -----------------------------------------------------------------------------