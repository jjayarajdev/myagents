"""
Module Name: human_agent.py

Description:
This module contains the implementation of a Human Agent using LangChain components. 
If a users input question is ambiguous and the llm cannot decide which agent can handle
it, then this agent comes into the picture and asks user to clarify his question.

"""

# -----------------------------------------------------------------------------
# SECTION: Imports
# -----------------------------------------------------------------------------

# Standard library imports
from typing import Union, Dict, List, Tuple
import json

# Third-party imports
from langchain_community.callbacks import get_openai_callback
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field

# Local application imports
from models.openai.azure_openai_model import llm_config_loader
from utils.helper_functions import load_prompt_yaml

# -----------------------------------------------------------------------------
# SECTION: Load Prompt
# -----------------------------------------------------------------------------

# Load the human agent system prompt from a file
human_agent_system_text = load_prompt_yaml(r"config_files\core_engine\human_agent_prompts\system_prompt.yaml")

# Create chat prompt template
human_agent_prompt = ChatPromptTemplate.from_messages(
    [("system", human_agent_system_text)]
)

# -----------------------------------------------------------------------------
# SECTION: LLM - Parser and Chain
# -----------------------------------------------------------------------------

class HumanAgent(BaseModel):
    """
    Pydantic model for the human agent response.

    Attributes:
        human_agent (str): Agents Response
    """
    human_agent: str = Field(description="Agents response")


# Create the output parser
parser = JsonOutputParser(pydantic_object=HumanAgent)

# Define the human agent chain
human_agent_chain = human_agent_prompt | llm_config_loader() | parser

# -----------------------------------------------------------------------------
# SECTION: Human Agent Function
# -----------------------------------------------------------------------------

def human_agent(function_params: Union[Dict[str, str], List[str]]) -> Tuple[str, int, int]:
    """
    Generate cross questions for more clarity on user input using the human agent.
 
    This function processes the user's question, looks into the conversation history,
    and based on that, it asks the user for more clarity on their input question.
 
    Args:
        function_params (Union[Dict[str, str], List[str]]): Either a dictionary with
        'input_text', 'conversation_history', and 'user_details' keys, or a list with input text.
 
    Returns:
        Tuple: A tuple containing:
            - str: The agent's response asking the user to clarify their input question.
            - int: The number of input tokens used by the AI model.
            - int: The number of output tokens generated by the AI model.
 
    Raises:
        ValueError: If invalid arguments are passed to the function.
    """
    print("%" * 50)
    print("In Human Agent")
    print(function_params)
    print("%" * 50)


    # Check if function_params is a dictionary and extract values
    if isinstance(function_params, dict):
        input_text = function_params.get('input_text', '')
        conversation_history = function_params.get('conversation_history', [])
        full_user_details = function_params.get('user_details', {})
    
    # If it's a list, assume it's just the input text
    elif isinstance(function_params, list) and len(function_params) > 0:
        input_text = function_params[0]
        conversation_history = []
        full_user_details = {}
    
    else:
        raise ValueError("Invalid input: Expected a dictionary with specific keys or a list with input text.")
 
    # Use OpenAI callback to capture token counts
    with get_openai_callback() as cb:
        ai_response = human_agent_chain.invoke({
            "full_user_details": json.dumps(full_user_details),
            "user_input": input_text,
            "conversation_history": conversation_history
        })
        print("AI Response human_agent: ", ai_response)
        input_tokens_count = cb.prompt_tokens
        output_tokens_count = cb.completion_tokens
 
    # Return the response along with input and output token counts
    return ai_response['human_agent'], input_tokens_count, output_tokens_count

# -----------------------------------------------------------------------------
# END OF MODULE
# -----------------------------------------------------------------------------